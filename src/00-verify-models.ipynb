{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b9b1da",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); color: white; padding: 20px; border-radius: 10px; box-shadow: 0 4px 8px rgba(0,0,0,0.2);\">\n",
    "    <h1 style=\"color: #FFF; text-shadow: 1px 1px 3px rgba(0,0,0,0.5);\">Step 1: Plan For Zava Scenario</h1>\n",
    "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
    "    Zava is an enterprise retailer that sells home improvement goods to DIY enthusiasts.\n",
    "    Cora is their AI customer support chatbot that helps customers find relevant products.\n",
    "    The Zava AI Engineering team wants to make sure Cora is: helpful, precise, and cost-effective to operate.\n",
    "    In this set of demos we'll learn how they approach the model customization journey, to make this happen.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8a1a23",
   "metadata": {},
   "source": [
    "## 1. Deploy Models\n",
    "\n",
    "This is \"Act 2\" of the breakout session where we look at core Fine-Tuning options in Azure AI Foundry. To achieve this:\n",
    "\n",
    "1. We need a base model to fine-tune - for SFT\n",
    "2. We need a teacher model and a student model to transfer behavioral knowledge - for Distillation.\n",
    "\n",
    "To achieve this, we have provisioned a few model \"candidates\" by default, using the setup process defined earlier. \n",
    "\n",
    "You should see some subset of these models in that list.\n",
    "\n",
    "- Reasoning Models ‚Üí o3, o3-mini, o4-mini\n",
    "- Chat Models ‚Üí  gpt-4o, gpt-4.1, gpt-4.1-nano\n",
    "- Embedding Models ‚Üí  text-embedding-3-large\n",
    "\n",
    "Your `.env` variables should already be set to reflect the desired Azure AI Foundry project environment. Let's go!\n",
    "\n",
    "\n",
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f640e1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e58d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Setup an Azure OpenAI client and test out different models\n",
    "import os\n",
    "import time\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\", \"2024-05-01-preview\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396a202",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 1. Define Test Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25becf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Define a test prompt that we'll use for all models\n",
    "test_prompt = \"\"\"\n",
    "You are a home improvement assistant for Zava, a fictional hardware store.\n",
    "Please give me a brief recommendation for a paint color for my living room.\n",
    "Include one key feature of the paint and a price range.\n",
    "\"\"\"\n",
    "\n",
    "# List of models to test\n",
    "models_to_test = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"o3\",\n",
    "    \"o3-mini\",\n",
    "    \"o4-mini\"\n",
    "]\n",
    "\n",
    "# Function to call a model and measure performance\n",
    "def test_model(model_name, prompt):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            \"model\": model_name,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are Cora, a polite, factual and helpful assistant for Zava, a DIY hardware store\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add the appropriate token limit parameter based on model type\n",
    "        if model_name.startswith(\"o\"):\n",
    "            params[\"max_completion_tokens\"] = 300\n",
    "        else:\n",
    "            params[\"max_tokens\"] = 300\n",
    "        \n",
    "        response = client.chat.completions.create(**params)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        \n",
    "        # Extract response and token usage\n",
    "        content = response.choices[0].message.content\n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "        \n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"latency\": latency,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"response\": content\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with model {model_name}: {str(e)}\")\n",
    "        return {\n",
    "            \"model\": model_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4f61e2",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 2. Run Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each model with the same prompt\n",
    "# Reset results to avoid duplicates when re-running this cell\n",
    "import sys\n",
    "\n",
    "print(\"üîÑ Starting model tests...\", flush=True)\n",
    "\n",
    "results = []\n",
    "detailed_outputs = {}\n",
    "\n",
    "for model in models_to_test:\n",
    "    try:\n",
    "        print(f\"..... Testing model: {model}\")\n",
    "        result = test_model(model, test_prompt)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception outside test_model for {model}: {str(e)}\", file=sys.stderr)\n",
    "        results.append({\"model\": model, \"error\": str(e)})\n",
    "\n",
    "# Store the detailed output for later, but don't display all of it \n",
    "for result in results:\n",
    "    if \"error\" not in result:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": result[\"response\"],\n",
    "            \"latency\": result[\"latency\"],\n",
    "            \"prompt_tokens\": result[\"prompt_tokens\"],\n",
    "            \"completion_tokens\": result[\"completion_tokens\"],\n",
    "            \"total_tokens\": result[\"total_tokens\"]\n",
    "        }\n",
    "    else:\n",
    "        detailed_outputs[result[\"model\"]] = {\n",
    "            \"response\": f\"ERROR: {result.get('error', 'Unknown error')}\",\n",
    "            \"latency\": None,\n",
    "            \"prompt_tokens\": None,\n",
    "            \"completion_tokens\": None,\n",
    "            \"total_tokens\": None\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úÖ Completed testing {len(detailed_outputs)} models\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef30d56",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "#### 3. Visualize Results\n",
    "\n",
    "You may see something like this (taken from a previous run) - note how the same prompt has different latency and token usage metrics for different models. In this instance, gpt-4.1 has the lowest total token usage (but the highest latency) - while the o3 reasoning model has the highest token usage (likely due to the reasoning tokens used). Now how the _gpt-4.1-nano_ has the lowest latency (with a slightly higher total token cost) bhile the gpt-4o model is in the middle.\n",
    "\n",
    "While these results are not conclusive, they offer us some intuition into two metrics (token usage and latency) that are key optimization targets for our assistant. **Note** that these results are _not_ grounded in Zava data (and therefore not accurate) - orchestrating a RAG-based solution would incur added token costs (to capture context in prompt) and latency (to retrieve and augment relevant results)\n",
    "\n",
    "| MODEL PERF METRICS | | | |\n",
    "|:---|:---|:---|:---|\n",
    "| Model | Latency (s) | Prompt Tokens |Completion Tokens | Total Tokens\n",
    "gpt-4o\t     | 1.36\t    | 74\t           |  93\t           |    167\n",
    "gpt-4.1\t     | 2.95\t    | 74\t            | 62\t            |    136\n",
    "gpt-4.1-nano | 1.10\t    | 74\t            | 76\t             |   150\n",
    "o3\t         | 1.81\t    | 73\t            | 144\t              |  217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ad974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(detailed_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd2d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ........ Now display the two clean tables\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import html\n",
    "\n",
    "# First table: Model Responses with left-aligned text\n",
    "response_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    # Truncate long responses for cleaner display\n",
    "    response = data[\"response\"]\n",
    "    if len(response) > 300:\n",
    "        response = response[:297] + \"...\"\n",
    "    response_data.append({\"Model\": model, \"Response\": response})\n",
    "\n",
    "response_df = pd.DataFrame(response_data)\n",
    "print(\"\\n\\nü§ñ MODEL RESPONSES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Display with proper escaping - using pandas built-in HTML rendering\n",
    "html_output = response_df.to_html(index=False, escape=True)\n",
    "html_output = html_output.replace('<td>ERROR', '<td style=\"color:red\">ERROR')\n",
    "html_output = html_output.replace('<table', '<table style=\"width:100%\"')\n",
    "html_output = html_output.replace('<th>', '<th style=\"text-align: left;\">')\n",
    "# Apply left alignment to all data cells\n",
    "html_output = html_output.replace('<td>', '<td style=\"text-align: left; vertical-align: top; padding: 8px;\">')\n",
    "display(HTML(html_output))\n",
    "\n",
    "# Second table: Performance Metrics\n",
    "metrics_data = []\n",
    "for model, data in detailed_outputs.items():\n",
    "    metrics_data.append({\n",
    "        \"Model\": model,\n",
    "        \"Latency (s)\": f\"{data['latency']:.2f}\" if data['latency'] is not None else \"N/A\",\n",
    "        \"Prompt Tokens\": data['prompt_tokens'] if data['prompt_tokens'] is not None else \"N/A\",\n",
    "        \"Completion Tokens\": data['completion_tokens'] if data['completion_tokens'] is not None else \"N/A\",\n",
    "        \"Total Tokens\": data['total_tokens'] if data['total_tokens'] is not None else \"N/A\"\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(\"\\n\\nüìä MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\"*100)\n",
    "display(HTML(metrics_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d14a5",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(135deg, #034694 0%, #1E8449 100%); color: white; padding: 15px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 20px 0;\">\n",
    "    <h2 style=\"color: #FFF; text-shadow: 1px 1px 2px rgba(0,0,0,0.4); margin: 0;\"> 2Ô∏è‚É£ | Understand The Requirements </h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258673f1",
   "metadata": {},
   "source": [
    "Our goal is to make the Cora chatbot **polite, factual, and helpful** to Zava shoppers. But what does this actually mean?\n",
    "\n",
    "1. **Polite & Helpful** - This is about changing the _tone_ and _style_ of responses from Cora to follow a desired template.\n",
    "1. **Factual** - This is about ensuring that responses are _grounded_ in Zava product data, typically using a RAG-based approach.\n",
    "\n",
    "**Desired Tone & Style**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f51ccc",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 3Ô∏è‚É£ | Explore The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146241e2",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 4Ô∏è‚É£ | Try Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d746e93",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 5Ô∏è‚É£ | Try Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0ff56",
   "metadata": {},
   "source": [
    "<div style=\"height: 6px; margin: 30px 0; background: linear-gradient(90deg, #034694 0%, #1E8449 50%, #D4AC0D 100%); border-radius: 3px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\"></div>\n",
    "\n",
    "## 6Ô∏è‚É£ | Time To Try Fine-Tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69820d",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: left; padding: 5px; height: 40px; background: linear-gradient(90deg, #7873f5 0%, #ff6ec4 100%); border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.12); font-size: 1.5em; font-weight: bold; color: #fff;\">\n",
    "   Next: Be More Helpful With SFT\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
